{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"toc\"></a>\n",
    "# Table of Contents\n",
    "## [Python for Data Analysis (pandas)](#pandas)\n",
    "## [Querying Dataframes](#querying)\n",
    "### [Query Challenge](#query_challenge)\n",
    "## [Reshaping Data](#reshaping)\n",
    "## [Aggregating Data](#aggregates)\n",
    "## [Merging Datasets](#merging)\n",
    "## [Ordering Data](#ordering)\n",
    "## [Exporting Data](#export)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"pandas\"></a>\n",
    "## Pandas\n",
    "[Back to Table of Contents](#toc)\n",
    "\n",
    "Pandas is a _library_ which allows us to do some powerful operations with table-like data. We can query datasets with a high degree of granularity, merge them together, sort, and aggregate them.\n",
    "\n",
    "I highly suggest you take a look and read through the documentation on it when you can! **[It is available here.](https://pandas.pydata.org/pandas-docs/stable/index.html)**\n",
    "\n",
    "Combined with this lesson are 4 Excel spreadsheets that we will be combining together to form various queries.\n",
    "\n",
    "- `COMPANIES.xlsx`\n",
    "- `EMPLOYEES.xlsx`\n",
    "- `FAKE_DATA_BUILD.xlsx` <- This is our main table\n",
    "- `ISO_COUNTRY_LOOKUPS.xlsx`\n",
    "\n",
    "To start with, let's read a spreadsheet and see the output.\n",
    "\n",
    "Remember, if we want to use a _library_ we have to **import** it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # we can create a temporary name using the \"as\" keyword here which can make it shorter\n",
    "\n",
    "pd.set_option('display.max_rows', 1000) # We can use this variable to decide how many rows we want to see\n",
    "pd.set_option('display.max_columns', 500) # We can use this variable to decide how many columns we want to see\n",
    "\n",
    "df_employees = pd.read_excel('EMPLOYEES.xlsx')\n",
    "df_employees # This will display the table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, I've named the table \"df_employees\". Since this table is going to hold the employee information, I've called it \"employees\" but I've added the prefix \"df_\", why?\n",
    "\n",
    "In pandas, table objects are stored as what are called \"dataframes\" - I'm using \"df\" as shorthand for that to let me know what kind of _object_ I'm working with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read our other sheets into dataframe objects so we can begin using them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main = pd.read_excel(\"FAKE_DATA_BUILD.xlsx\", sheet_name=\"Main\")\n",
    "df_main_definitions = pd.read_excel(\"FAKE_DATA_BUILD.xlsx\", sheet_name=\"VariableDefinitions\")\n",
    "df_companies = pd.read_excel(\"COMPANIES.xlsx\")\n",
    "df_countries = pd.read_excel(\"ISO_COUNTRY_LOOKUPS.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write each dataframe name here individually and see what it outputs when you run this\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can learn a little bit about our data using a few small tools:\n",
    "\n",
    "- `len()` <- tells us how many rows are in the dataframe.\n",
    "- `dataframe.info()` <- gives us basic information about the dataframe.\n",
    "- `dataframe.columns.values` <- Gives us the headers of the dataframe (useful later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of rows in df_main: ' + str(len(df_main)))\n",
    "print('Number of rows in df_employees: ' + str(len(df_employees)))\n",
    "print('Number of rows in df_companies: ' + str(len(df_companies)))\n",
    "print('Number of rows in df_countries: ' + str(len(df_countries)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at df_main and see what we can learn about our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although looking at the data is useful, we need to learn a little more about what each column contains. In this dataset, we have a \"VariableDefinitions\" sheet what we pulled into df_main_definitions. Let's take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_main_definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"querying\"></a>\n",
    "## Querying Dataframes\n",
    "[Back to Table of Contents](#toc)\n",
    "\n",
    "Reading data is useful but without the ability to ask a dataset questions, it doesn't really give us much more over Excel. This is where the strength of pandas starts to show.\n",
    "\n",
    "The _syntax_ for querying is fairly simple but can get complex based upon the query itself.\n",
    "\n",
    "For our purposes, I'll describe it this way:\n",
    "\n",
    "The statement: `df_main[df_main[\"ORGN_CTRY_CODE\"] == \"BR\"]` can be read as \"within the dataframe df_main tell me where 'ORGN_CTRY_CODE' equals 'BR'.\" Or, more simply: all data for exports from Brazile.\n",
    "\n",
    "We can also utilize multiple conditions like so:\n",
    "\n",
    "`df_main[(df_main[\"ORGN_CTRY_CODE\"] == \"BR\") & (df_main[\"DEST_CTRY_CODE\"] == \"US\")]` - this can be read as \"within the dataframe df_main tell me where 'ORGN_CTRY_CODE' equals 'BR' **_and_** 'DEST_CTRY_CODE' equals 'US'. Or, more simply, all data for packages exported from Brazil to the US.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main[df_main[\"ORGN_CTRY_CODE\"] == \"BR\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main[(df_main[\"ORGN_CTRY_CODE\"] == \"BR\") & (df_main[\"DEST_CTRY_CODE\"] == \"US\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"query_challenge\"></a>\n",
    "### Query Challenge\n",
    "[Back to Table of Contents](#toc)\n",
    "\n",
    "Write the following queries:\n",
    "\n",
    "- Get information about packages _from_ Mexico that were of transportation type \"Truck\"\n",
    "- Get information about packages from Argentina whose invoices have _NOT_ been paid.\n",
    "- Get information about packages from the US for 2017\n",
    "    - Query dates like this: `(df_main[\"DATETIME\"] >= '01-01-2017') & (df_main[\"DATETIME\"] < '01-01-2018')`\n",
    "- Get information about packages for all countries whose \"SOLUTION_TYPE\" was \"FF\" that have \"INVOICE_REV\" of greater than 2500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"reshaping\"></a>\n",
    "## Reshaping Data\n",
    "[Back to Table of Contents](#toc)\n",
    "\n",
    "Remember viewing the output of `dataframe.columns.values`? Let's use that to slim down the actual data we want for display.\n",
    "\n",
    "We can get a subset of columns of a dataframe like this:\n",
    "\n",
    "- `df_new` has headers `[\"col1\", \"col2\", \"col3\", \"col4\"]`.\n",
    "- We get a subset of these columns by creating a list of the columns we want and feeding it to the dataframe.\n",
    "    - `df_new_subset = df_new[[\"col2\", \"col4\"]]`\n",
    "\n",
    "Let's get the headers of `df_main` again and take a look to see which ones we want - we'll convert them to a list as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdrs_main = list(df_main.columns.values)\n",
    "hdrs_main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say our end goal is to get the total revenue for each employee. What columns would we need to do that? We'd probably need the employee ID and the invoice revenue - two columns \"EMPLOYEE_ID\" and \"INVOICE_REV\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_employee_revenue = df_main[[\"EMPLOYEE_ID\", \"INVOICE_REV\"]]\n",
    "df_employee_revenue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"aggregates\"></a>\n",
    "## Aggregating Data\n",
    "[Back to Table of Contents](#toc)\n",
    "\n",
    "It's great that we can get the columns that we need but there are two problems.\n",
    "1. This is each individual transaction, it doesn't help us see the _total_ revenue for each employee.\n",
    "2. We don't know who any of these employee IDs refer to (more on this later).\n",
    "\n",
    "First, we need to _aggregate_ the data by summing the revenue for each employee.\n",
    "\n",
    "The syntax is as follows:\n",
    "\n",
    "`df_employee_revenue.groupby([\"EMPLOYEE_ID\"])[\"INVOICE_REV\"].sum()`\n",
    "\n",
    "We can also get the _average_ by using a different function:\n",
    "\n",
    "`df_employee_revenue.groupby([\"EMPLOYEE_ID\"])[\"INVOICE_REV\"].mean()`\n",
    "\n",
    "Let's try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tot_emp_rev = df_employee_revenue.groupby([\"EMPLOYEE_ID\"], as_index=False)[\"INVOICE_REV\"].sum()\n",
    "df_tot_emp_rev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"merging\"></a>\n",
    "## Merging Datasets\n",
    "[Back to Table of Contents](#toc)\n",
    "\n",
    "Aggregating data is useful but it doesn't solve the 2nd problem we mentioned earlier: we don't know who these people are. Luckily, we have another spreadsheet which has all of the employee information we need to solve this problem.\n",
    "\n",
    "The way that we handle situations like this is to use pandas' _merge_ functionality.\n",
    "\n",
    "Here's the syntax:\n",
    "\n",
    "`df1.merge(df2, left_on='lkey', right_on='rkey')`\n",
    "\n",
    "- **df1** is the dataset you want to merge outside data onto.\n",
    "- **df2** is the dataset you want to merge onto df1.\n",
    "- **left_on** is the name of the column in df1 you want df2 to merge onto.\n",
    "- **right_on** is the name of the column in df2 you want to connect to the column in df1.\n",
    "- If you don't put anything for the keys, it will attempt to find a matching pair of keys.\n",
    "\n",
    "[Additional information can be found here.](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.merge.html)\n",
    "\n",
    "Let's merge df_employees onto df_employee_revenue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at df_employees again to verify the columns and data\n",
    "df_employees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tot_emp_rev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's merge\n",
    "df_employee_rev_info = df_tot_emp_rev.merge(df_employees, left_on=\"EMPLOYEE_ID\", right_on=\"EMPLOYEE_ID\")\n",
    "\n",
    "df_employee_rev_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ordering\"></a>\n",
    "## Ordering Data\n",
    "[Back to Table of Contents](#toc)\n",
    "\n",
    "Now that we've aggregated and merged out data, let's order our data and find the top 10 employees in our company! [More information is available here.](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sort_values.html)\n",
    "\n",
    "The syntax for sorting values is as follows:\n",
    "\n",
    "`df.sort_values(by=['col1'])`\n",
    "\n",
    "The column we want to sort by is \"INVOICE_REV\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_employee_rev_info = df_employee_rev_info.sort_values(by=[\"INVOICE_REV\"], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get the top 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_top_10 = df_employee_rev_info.head(10)\n",
    "df_top_10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"exporting\"></a>\n",
    "## Exporting Data\n",
    "[Back to Table of Contents](#toc)\n",
    "\n",
    "Now that we've created our report, let's export it into Excel!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_top_10.to_excel(\"our_cool_employee_report.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
